<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>壁纸分享</title>
      <link href="/2020/06/18/test/"/>
      <url>/2020/06/18/test/</url>
      
        <content type="html"><![CDATA[<img src="../images/image-20200618200057678.png" alt="image-20200618200057678" style="zoom: 33%;"><p><img src="https://cdn.jsdelivr.net/gh/ljw919/photos/imgs/bg.jpg" alt></p><img src="https://cdn.jsdelivr.net/gh/ljw919/photos/imgs/bg.jpg" style="zoom:33%;">]]></content>
      
      
      <categories>
          
          <category> fun </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 壁纸 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch-lstm</title>
      <link href="/2020/06/18/pytorch-lstm/"/>
      <url>/2020/06/18/pytorch-lstm/</url>
      
        <content type="html"><![CDATA[<h2 id="LSTM时间序列预测模型"><a href="#LSTM时间序列预测模型" class="headerlink" title="LSTM时间序列预测模型"></a>LSTM时间序列预测模型</h2><ol><li><p>长短期记忆（long short-term memory，LSTM）。本节将基于pytorch建立一个LSTM模型，以用于航班乘客数据的预测，这里将直接按照代码块进行解释。</p><p><a href="https://stackabuse.com/time-series-prediction-using-lstm-with-pytorch-in-python/" target="_blank" rel="noopener">https://stackabuse.com/time-series-prediction-using-lstm-with-pytorch-in-python/</a></p><a id="more"></a></li><li><p>数据的预处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#时间序列预测模型LSTM</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns  <span class="comment">#读取seaborn的数据文件，需要ladder</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"></span><br><span class="line">flight_data = sns.load_dataset(<span class="string">"flights"</span>)</span><br><span class="line"><span class="comment">#print(flight_data.head())</span></span><br><span class="line">all_data = flight_data[<span class="string">'passengers'</span>].values.astype(float)</span><br><span class="line"></span><br><span class="line"><span class="comment">#一共144行数据，这里设置每12个数据为一个间隔（1年为12月）</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#任务是根据前132个月来预测最近12个月内旅行的乘客人数。请记住，我们有144个月的记录，这意味着前132个月的数据将用于训练我们的LSTM模型，而模型性能将使用最近12个月的值进行评估。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#最后待验证的数据集（月份数目）大小，数值可以修改，记得修改最后plot对应的x范围即可</span></span><br><span class="line">test_data_size = <span class="number">12</span>   </span><br><span class="line">train_data = all_data[:-test_data_size] <span class="comment">#训练数据</span></span><br><span class="line">test_data = all_data[-test_data_size:]  <span class="comment">#测试数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#归一化处理减小误差</span></span><br><span class="line">scaler = MinMaxScaler(feature_range=(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">train_data_normalized = scaler.fit_transform(train_data.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">train_data_normalized = torch.FloatTensor(train_data_normalized).view(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#重点  创建读取的数据列表。</span></span><br><span class="line"><span class="comment">#每组数据有2个元组。前一个元组有12个月份的数据，后一个元组只有一个元素，表示第13个月份的数据，用于和基于前12个数据预测的数据求loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_inout_sequences</span><span class="params">(input_data, tw)</span>:</span></span><br><span class="line">    inout_seq = []</span><br><span class="line">    L = len(input_data)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(L-tw):  <span class="comment">#L-tw = 144-32 = 132个数据</span></span><br><span class="line">        train_seq = input_data[i:i+tw]  <span class="comment">#12个数据一组，进行训练</span></span><br><span class="line">        train_label = input_data[i+tw:i+tw+<span class="number">1</span>]  <span class="comment">#第12+1个数据作为label计算loss</span></span><br><span class="line">        inout_seq.append((train_seq, train_label))  </span><br><span class="line">    <span class="keyword">return</span> inout_seq</span><br><span class="line"></span><br><span class="line"><span class="comment">#这是每次训练的数据（月份数目），设置为12个月，可以进行调整</span></span><br><span class="line">train_window = <span class="number">12</span>  <span class="comment">#tw，设置训练输入的序列长度为12</span></span><br><span class="line">train_inout_seq = create_inout_sequences(train_data_normalized, train_window)</span><br></pre></td></tr></table></figure><p>数据集结果如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">train_inout_seq[:<span class="number">5</span>]</span><br><span class="line">[(tensor([<span class="number">-0.9648</span>, <span class="number">-0.9385</span>, <span class="number">-0.8769</span>, <span class="number">-0.8901</span>, <span class="number">-0.9253</span>, <span class="number">-0.8637</span>, <span class="number">-0.8066</span>, <span class="number">-0.8066</span>,</span><br><span class="line">          <span class="number">-0.8593</span>, <span class="number">-0.9341</span>, <span class="number">-1.0000</span>, <span class="number">-0.9385</span>]), tensor([<span class="number">-0.9516</span>])),</span><br><span class="line"> </span><br><span class="line"> (tensor([<span class="number">-0.9385</span>, <span class="number">-0.8769</span>, <span class="number">-0.8901</span>, <span class="number">-0.9253</span>, <span class="number">-0.8637</span>, <span class="number">-0.8066</span>, <span class="number">-0.8066</span>, <span class="number">-0.8593</span>,</span><br><span class="line">          <span class="number">-0.9341</span>, <span class="number">-1.0000</span>, <span class="number">-0.9385</span>, <span class="number">-0.9516</span>]),tensor([<span class="number">-0.9033</span>])),</span><br><span class="line"> </span><br><span class="line"> (tensor([<span class="number">-0.8769</span>, <span class="number">-0.8901</span>, <span class="number">-0.9253</span>, <span class="number">-0.8637</span>, <span class="number">-0.8066</span>, <span class="number">-0.8066</span>, <span class="number">-0.8593</span>, <span class="number">-0.9341</span>,</span><br><span class="line">          <span class="number">-1.0000</span>, <span class="number">-0.9385</span>, <span class="number">-0.9516</span>, <span class="number">-0.9033</span>]), tensor([<span class="number">-0.8374</span>])),</span><br><span class="line"> </span><br><span class="line"> (tensor([<span class="number">-0.8901</span>, <span class="number">-0.9253</span>, <span class="number">-0.8637</span>, <span class="number">-0.8066</span>, <span class="number">-0.8066</span>, <span class="number">-0.8593</span>, <span class="number">-0.9341</span>, <span class="number">-1.0000</span>,</span><br><span class="line">          <span class="number">-0.9385</span>, <span class="number">-0.9516</span>, <span class="number">-0.9033</span>, <span class="number">-0.8374</span>]), tensor([<span class="number">-0.8637</span>])),</span><br><span class="line"> </span><br><span class="line"> (tensor([<span class="number">-0.9253</span>, <span class="number">-0.8637</span>, <span class="number">-0.8066</span>, <span class="number">-0.8066</span>, <span class="number">-0.8593</span>, <span class="number">-0.9341</span>, <span class="number">-1.0000</span>, <span class="number">-0.9385</span>,</span><br><span class="line">          <span class="number">-0.9516</span>, <span class="number">-0.9033</span>, <span class="number">-0.8374</span>, <span class="number">-0.8637</span>]), tensor([<span class="number">-0.9077</span>]))]</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据每次移动一个位置，每移动一次，上一个列表的第二个数据成为下一个列表的第一个数据，标签label也依次向后挪动。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#打印train_inout_seq列表的长度，将看到它包含120个项目。这是因为尽管训练集包含132个元素，但是序列长度为12，这意味着第一个序列由前12个项目组成，第13个项目是第一个序列的标签。同样，第二个序列从第二个项目开始，到第13个项目结束，而第14个项目是第二个序列的标签，依此类推。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#包含120个项目，因为如果继续从第120个数据读取到132个数据，那么需要第133个数据来求loss，已经超出范围，无法满足了，所以最多120行。</span></span><br></pre></td></tr></table></figure></li><li><p>LSTM网络搭建</p><p>​     input_size：对应于输入中的要素数量。尽管我们的序列长度为12，但每个月我们只有1个值，即乘客总数，因此输入大小为1。 </p><p>​     hidden_layer_size：指定隐藏层的数量以及每层中神经元的数量。                  </p><p>​     output_size：输出中的项目数，由于我们要预测未来1个月的乘客人数，因此输出大小为1。  </p><p>​     在构造函数中，我们创建变量hidden_layer_size，lstm，linear和hidden_cell。LSTM算法接受三个输入：先前的隐藏状态，先前的单元状态和当前输入。该hidden_cell变量包含先前的隐藏状态和单元状态。lstm和linear层变量用于创建LSTM和线性层。</p><p>​    在forward方法内部，将input_seq作为参数传递给lstm图层。lstm层的输出是当前时间步的隐藏状态和单元状态以及输出。lstm图层的输出将传递到该linear图层。预计的乘客人数存储在predictions列表的最后一项中，并返回到调用函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTM</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size=<span class="number">1</span>, hidden_layer_size=<span class="number">100</span>, output_size=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.hidden_layer_size = hidden_layer_size</span><br><span class="line"></span><br><span class="line">        self.lstm = nn.LSTM(input_size, hidden_layer_size)  <span class="comment">#lstm层</span></span><br><span class="line"></span><br><span class="line">        self.linear = nn.Linear(hidden_layer_size, output_size)  <span class="comment">#全连接层</span></span><br><span class="line"></span><br><span class="line">        self.hidden_cell = (torch.zeros(<span class="number">1</span>,<span class="number">1</span>,self.hidden_layer_size), <span class="comment">#hidden_cell层</span></span><br><span class="line">                            torch.zeros(<span class="number">1</span>,<span class="number">1</span>,self.hidden_layer_size))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_seq)</span>:</span></span><br><span class="line">        <span class="comment">#lstm处理序列数据，并传递到hidden_cell，输出lstm_out</span></span><br><span class="line">        <span class="comment">#输入数据格式：input(seq_len, batch, input_size)</span></span><br><span class="line">        <span class="comment">#seq_len：每个序列的长度</span></span><br><span class="line">        <span class="comment">#batch_size:设置为1</span></span><br><span class="line">        <span class="comment">#input_size:输入矩阵特征数</span></span><br><span class="line">        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq) ,<span class="number">1</span>, <span class="number">-1</span>), self.hidden_cell)</span><br><span class="line">        <span class="comment">#全连接层输出predictions</span></span><br><span class="line">        predictions = self.linear(lstm_out.view(len(input_seq), <span class="number">-1</span>))</span><br><span class="line">        <span class="keyword">return</span> predictions[<span class="number">-1</span>]</span><br></pre></td></tr></table></figure></li></ol><ol start="4"><li><p>训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">model = LSTM()</span><br><span class="line">loss_function = nn.MSELoss() <span class="comment">#损失函数</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)  <span class="comment">#优化器</span></span><br><span class="line"></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="keyword">for</span> seq, labels <span class="keyword">in</span> train_inout_seq:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#补充作用？</span></span><br><span class="line">        model.hidden_cell = (torch.zeros(<span class="number">1</span>,<span class="number">1</span>,model.hidden_layer_size),torch.zeros(<span class="number">1</span>,<span class="number">1</span>,model.hidden_layer_size))</span><br><span class="line">        </span><br><span class="line">        y_pred = model(seq)</span><br><span class="line">        single_loss = loss_function(y_pred, labels)</span><br><span class="line">        single_loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">25</span> == <span class="number">1</span>:</span><br><span class="line">        print(<span class="string">f'epoch:<span class="subst">&#123;i:<span class="number">3</span>&#125;</span>loss: <span class="subst">&#123;single_loss.item():<span class="number">10.8</span>f&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f'epoch: <span class="subst">&#123;i:<span class="number">3</span>&#125;</span> loss: <span class="subst">&#123;single_loss.item():<span class="number">10.10</span>f&#125;</span>'</span>)</span><br></pre></td></tr></table></figure></li><li><p>预测（利用前12个点预测新点，然后将预测值作为新的输入，滚动预测下一个点）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">fut_pred = <span class="number">12</span>  <span class="comment">#预测fut_pred个数据  </span></span><br><span class="line">test_inputs = train_data_normalized[-train_window:].tolist()</span><br><span class="line"><span class="comment">#取出最后12个月数据为列表 待预测的数据是第133个起</span></span><br><span class="line">model.eval()  <span class="comment">#eval模式，不更新梯度</span></span><br><span class="line"><span class="comment">#预测133~（133+11）个数据  12</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(fut_pred):</span><br><span class="line">    seq = torch.FloatTensor(test_inputs[-train_window:])  <span class="comment">#取出最后12个数据</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        model.hidden_cell = (torch.zeros(<span class="number">1</span>,<span class="number">1</span>,model.hidden_layer_size),torch.zeros(<span class="number">1</span>,<span class="number">1</span>,model.hidden_layer_size))</span><br><span class="line">        test_inputs.append(model(seq).item())  <span class="comment">#每次输出一个预测值，加到列表</span></span><br><span class="line"></span><br><span class="line">print(test_inputs[train_window:])  <span class="comment">#train_window起才是预测的数据(训练集中最后一组训练数据)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#逆归一化回原值范围,test_inputs中排除前12个数据（训练数据）</span></span><br><span class="line">actual_predictions = scaler.inverse_transform(np.array(test_inputs[train_window:] ).reshape(<span class="number">-1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure></li><li><p>绘图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(<span class="number">132</span>, <span class="number">132</span>+fut_pred, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">'Month vs Passenger'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Total Passengers'</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.autoscale(axis=<span class="string">'x'</span>, tight=<span class="literal">True</span>)</span><br><span class="line">plt.plot(np.arange(<span class="number">132</span>,<span class="number">144</span>,<span class="number">1</span>), test_data)  <span class="comment">#test_data只有12个数据真实值</span></span><br><span class="line">plt.plot(x, actual_predictions)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="/.com//Users\LENOVO\AppData\Roaming\Typora\typora-user-images\image-20200612184554673.png" alt="image-20200612184554673" style="zoom: 50%;"></li></ol><h2 id="TIPS"><a href="#TIPS" class="headerlink" title="TIPS"></a>TIPS</h2><p>介绍几个参数</p><ol><li><p>test_data_size : 原始数据集中最后待验证的数据个数，可修改</p></li><li><p>train_window : 训练数据集中，每批次训练的数据个数，可修改</p></li><li><p>fut_pred : 待预测的数据个数</p></li><li><pre><code class="python">LSTM输入数据格式：input(seq_len, batch_size, input_size)<span class="comment">#seq_len/timestep：每个序列的长度</span><span class="comment">#batch_size:设置为1</span><span class="comment">#input_size:输入矩阵特征数</span></code></pre></li></ol><p>这里我们可以随意的修改，比如可以修改成用最后14个数据验证，每次用15个月份的数据作训练，预测100个月份的数据等等。当然一般来说，不同的问题下，最佳设置的参数不同，比如本问题中就是按1年12个月来设置是最佳的结果。</p>]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2020/06/18/hello-world/"/>
      <url>/2020/06/18/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><a id="more"></a><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
